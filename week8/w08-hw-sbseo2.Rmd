---
title: "Week 8 - Homework"
author: "STAT 420, Summer 2020, Bruno Seo (sbseo2)"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.alin = "center")
```

## Exercise 1 (Writing Functions)

**(a)** Write a function named `diagnostics` that takes as input the arguments:

- `model`, an object of class `lm()`, that is a model fit via `lm()`
- `pcol`, for controlling point colors in plots, with a default value of `grey`
- `lcol`, for controlling line colors in plots, with a default value of `dodgerblue`
- `alpha`, the significance level of any test that will be performed inside the function, with a default value of `0.05`
- `plotit`, a logical value for controlling display of plots with default value `TRUE`
- `testit`, a logical value for controlling outputting the results of tests with default value `TRUE`

The function should output:

- A list with two elements when `testit` is `TRUE`:
    - `p_val`, the p-value for the Shapiro-Wilk test for assessing normality
    - `decision`, the decision made when performing the Shapiro-Wilk test using the `alpha` value input to the function. "Reject" if the null hypothesis is rejected, otherwise "Fail to Reject."
- Two plots, side-by-side, when `plotit` is `TRUE`:
    - A fitted versus residuals plot that adds a horizontal line at $y = 0$, and labels the $x$-axis "Fitted" and the $y$-axis "Residuals." The points and line should be colored according to the input arguments. Give the plot a title. 
    - A Normal Q-Q plot of the residuals that adds the appropriate line using `qqline()`. The points and line should be colored according to the input arguments. Be sure the plot has a title. 

Consider using this function to help with the remainder of the assignment as well.

```{r}
diagnostics = function(model, pcol='grey', lcol='dodgerblue', alpha=.05, plotit=TRUE, testit=TRUE) {
  
  if (plotit) {
    par(mfrow = c(1,2))
    
    plot(fitted(model), resid(model), col=pcol, main='Fitted vs residuals', ylab='Residuals', xlab='Fitted values')
    abline(h=0, col=lcol, lwd=2)
    
    qqnorm(resid(model), col=pcol, main="Normal Q-Q plot")
    qqline(resid(model), col=lcol, lwd=2)
  }
  
  if (testit) {
    p_val = shapiro.test(resid(model))$p.value
    decision = "Fail to Reject"  
    if (alpha > p_val){ decision = "Reject" }
    
    list(p_val=p_val, decision=decision)
  }
}

```


**(b)** Run the following code.

```{r}
set.seed(40)

data_1 = data.frame(x = runif(n = 30, min = 0, max = 10),
                    y = rep(x = 0, times = 30))
data_1$y = with(data_1, 2 + 1 * x + rexp(n = 30))
fit_1 = lm(y ~ x, data = data_1)

data_2 = data.frame(x = runif(n = 20, min = 0, max = 10),
                    y = rep(x = 0, times = 20))
data_2$y = with(data_2, 5 + 2 * x + rnorm(n = 20))
fit_2 = lm(y ~ x, data = data_2)

data_3 = data.frame(x = runif(n = 40, min = 0, max = 10),
                    y = rep(x = 0, times = 40))
data_3$y = with(data_3, 2 + 1 * x + rnorm(n = 40, sd = x))
fit_3 = lm(y ~ x, data = data_3)
```

```{r}
diagnostics(fit_1, plotit = FALSE)$p_val
diagnostics(fit_2, plotit = FALSE)$decision
diagnostics(fit_1, testit = FALSE, pcol = "black", lcol = "black")
diagnostics(fit_2, testit = FALSE, pcol = "grey", lcol = "green")
diagnostics(fit_3)
```

***

## Exercise 2 (Prostate Cancer Data)

For this exercise, we will use the `prostate` data, which can be found in the `faraway` package. After loading the `faraway` package, use `?prostate` to learn about this dataset.

```{r, message = FALSE, warning = FALSE}
library(faraway)
```

**(a)** Fit an additive multiple regression model with `lpsa` as the response and the remaining variables in the `prostate` dataset as predictors. Report the $R^2$ value for this model.

```{r}
prostate_model = lm(lpsa~., data=prostate)
summary(prostate_model)$r.squared
```


**(b)** Check the constant variance assumption for this model. Do you feel it has been violated? Justify your answer.

```{r}
library(lmtest)
bptest(prostate_model)$p.value
```

$$ H_0: \text{Homoscedasticity} $$
$$ H_1: \text{Hetroscedasticity} $$

No, constant variance assumption does not seem to be violated. 
That is because bptest fails to reject null hypothesis at level $\alpha <= 0.25$.

**(c)** Check the normality assumption for this model. Do you feel it has been violated? Justify your answer.

```{r}
diagnostics(prostate_model)
```

$$H_0: \text{Normality assumption is not suspect}$$
$$H_1: \text{Normality assumption is suspect}$$

Based on the Shapiro-Wilk normality test, I do not think normality assumption is violated. `p-value` is large enough not to reject null hypothesis. 

In addition, `fitted vs residuals plot` shows the spread of the residuals are roughly the same.

Finally, `normal q-q plot` shows that the points of the plot closely follow the straight line.

**(d)** Check for any high leverage observations. Report any observations you determine to have high leverage.

$$ h_i > 2\bar{h} $$
We assume that high leverage values are those with greater than two times of mean leverage. 

```{r}
hatvalues(prostate_model)[hatvalues(prostate_model) > 2*mean(hatvalues(prostate_model))]
```
We have observed five observations above have high leverage. Note that these are the observations that have potential to have high influence.

**(e)** Check for any influential observations. Report any observations you determine to be influential.

$$D_i > \frac{4}{n}$$

A cook's distance is often considered large if the condition above is met. 

```{r}
cooks.distance(prostate_model)[cooks.distance(prostate_model) > (4 / length(cooks.distance(prostate_model)))]
```

Above are the indices of observations that are highly influential. Note that visualization below shows that index number 39, 69, and 95 are repetitively found.

```{r}
par(mfrow=c(2,2))
plot(prostate_model)
```


**(f)** Refit the additive multiple regression model without any points you identified as influential. Compare the coefficients of this fitted model to the previously fitted model.

```{r}
cd = cooks.distance(prostate_model)
prostate_add_model = lm(lpsa~., data=prostate, subset= cd <= 4/length(cd))

# Previously fitted model
sum(coef(prostate_model))
# New Fitted model without influential observations
sum(coef(prostate_add_model))
```

The result shows that the sum of coefficients of new fitted model without influential observations are smaller than that of previously fitted model.

**(g)** Create a data frame that stores the observations that were "removed" because they were influential. Use the two models you have fit to make predictions with these observations. Comment on the difference between these two sets of predictions.

```{r}
influenctial_idx = which(cooks.distance(prostate_model) > (4 / length(cooks.distance(prostate_model))))

df_without_influential = prostate[-influenctial_idx, ]
model_without_influential = lm(lpsa~., data=df_without_influential)

all.equal(coef(prostate_add_model), coef(prostate_add_model))
```
Two addictive models have been used for comparison. One is an addictive model without modifying the original data. The other is an addictive model where the original data are modified
The result turns out that these two models are exactly the same when it comes to prediction. That is because all of the coefficeints are exactly the same.

***

## Exercise 3 (Why Bother?)

**Why** do we care about violations of assumptions? One key reason is that the distributions of the parameter esimators that we have used are all reliant on these assumptions. When the assumptions are violated, the distributional results are not correct, so our tests are garbage. **Garbage In, Garbage Out!**

Consider the following setup that we will use for the remainder of the exercise. We choose a sample size of 50.

```{r}
n = 50
set.seed(420)
x_1 = runif(n, 0, 5)
x_2 = runif(n, -2, 2)
```

Consider the model,

\[
Y = 4 + 1 x_1 + 0 x_2 + \epsilon.
\]

That is,

- $\beta_0$ = 4
- $\beta_1$ = 1
- $\beta_2$ = 0

We now simulate `y_1` in a manner that does **not** violate any assumptions, which we will verify. In this case $\epsilon \sim N(0, 1).$

```{r}
set.seed(83)
library(lmtest)

simulate_fit_1 = function(){
  y_1 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = 1)
  fit_1 = lm(y_1 ~ x_1 + x_2)
  summary(fit_1)$coefficients[3,4]
}
```

Then, we simulate `y_2` in a manner that **does** violate assumptions, which we again verify. In this case $\epsilon \sim N(0, \sigma = |x_2|).$

```{r}
set.seed(83)
simulate_fit_2 = function(){
  y_2 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = abs(x_2))
  fit_2 = lm(y_2 ~ x_1 + x_2)
  summary(fit_2)$coefficients[3,4]
}
```


**(a)** Use the following code after changing `birthday` to your birthday.

```{r}
num_sims = 2500
p_val_1 = rep(0, num_sims)
p_val_2 = rep(0, num_sims)
birthday = 19891210
set.seed(birthday)
```

Repeat the above process of generating `y_1` and `y_2` as defined above, and fit models with each as the response `2500` times. Each time, store the p-value for testing,

\[
\beta_2 = 0,
\]

using both models, in the appropriate variables defined above. (You do not need to use a data frame as we have in the past. Although, feel free to modify the code to instead use a data frame.)

```{r}
for (i in 1:num_sims){
  p_val_1[i] = simulate_fit_1()
  p_val_2[i] = simulate_fit_2()
}
```




**(b)** What proportion of the `p_val_1` values is less than 0.01? Less than 0.05? Less than 0.10? What proportion of the `p_val_2` values is less than 0.01? Less than 0.05? Less than 0.10? Arrange your results in a table. Briefly explain these results.

```{r}
df_proportion = data.frame(
                          alpha = c(0.01, 0.05, 0.10),
                          p_val_1 = c(
                          length(p_val_1[p_val_1 < 0.01]) / length(p_val_1),
                          length(p_val_1[p_val_1 < 0.05]) / length(p_val_1),
                          length(p_val_1[p_val_1 < 0.10]) / length(p_val_1)),
                          p_val_2 = c(
                          length(p_val_2[p_val_2 < 0.01]) / length(p_val_2),
                          length(p_val_2[p_val_2 < 0.05]) / length(p_val_2),
                          length(p_val_2[p_val_2 < 0.10]) / length(p_val_2)))


library(knitr)
kable(df_proportion)
```

Simulation result shows that we obtain the `p-values` for $\beta_2$ as expected. Since $\beta_2 = 0$, we expect roughly $1\%$ of the p-values to be significant at $\alpha=0.01$. The first model where variance assumption is not violated meets this expectation. However, the second model where the variance assumption is not satisfied fails to explain the signifance of $\beta_2$. In short, garbage-in, garbage-out.

***
## Exercise 4 (Corrosion Data)

For this exercise, we will use the `corrosion` data, which can be found in the `faraway` package. After loading the `faraway` package, use `?corrosion` to learn about this dataset.

```{r, message = FALSE, warning = FALSE}
library(faraway)
```

**(a)** Fit a simple linear regression with `loss` as the response and `Fe` as the predictor. Plot a scatterplot and add the fitted line. Check the assumptions of this model.

```{r}
corrosion_model = lm(loss~Fe, data=corrosion)


plot(loss~Fe, data=corrosion, col="grey", main='Scatter plot')
abline(corrosion_model, col="orange", lwd=3)
par(mfrow=c(1,2))
diagnostics(corrosion_model, testit = FALSE)
bptest(corrosion_model)
diagnostics(corrosion_model, plotit = FALSE)
```

All of the plots above show that the simple linear regression fitted to `Corrosion data` meet the following assumptions.


From the first plot where y is `response` and x is `predictor`, it shows that this fitted model follows the `linear` assumption. The `fitted vs residuals plot` shows that `equal variance` assumption is satisified. The p-value of `BP test` also supports that the `equal variance` assumption is met. Finally, `normal qq plot` visualizes that `normal distribution` assumption is met.  `Shapriro test` also agrees with this decision.

**(b)** Fit higher order polynomial models of degree 2, 3, and 4. For each, plot a fitted versus residuals plot and comment on the constant variance assumption. Based on those plots, which of these three models do you think are acceptable? Use a statistical test(s) to compare the models you just chose. Based on the test, which is preferred? Check the normality assumption of this model. Identify any influential observations of this model.

```{r}
corrosion_model2 = lm(loss~Fe + I(Fe^2), data=corrosion)
coro_pval_2 = diagnostics(corrosion_model2)$p_val
coro_bp_2 = bptest(corrosion_model2)$p.value
```
```{r}
corrosion_model3 = lm(loss~Fe + I(Fe^2)+I(Fe^3), data=corrosion)
coro_pval_3 = diagnostics(corrosion_model3)$p_val
coro_bp_3 = bptest(corrosion_model3)$p.value

```
```{r}
corrosion_model4 = lm(loss~Fe + I(Fe^2)+ I(Fe^3) + I(Fe^4), data=corrosion)
coro_pval_4 = diagnostics(corrosion_model4)$p_val
coro_bp_4 = bptest(corrosion_model4)$p.value
```

```{r}
coro_df = data.frame(degree = c('2','3','4'),
                  pval_variance = c(coro_bp_2, coro_bp_3, coro_bp_4),
                  pval_normal = c(coro_pval_2, coro_pval_3, coro_pval_4))

kable(coro_df)
```

$$H_0: Homoscedasticity$$
$$H_1: Hetroscedasticity$$

At $\alpha = 0.05$, the `p-vlaue` obtained from `bp-test` shows that the models of degree 2, 3, 4 all succeeds in meeting constant variance assumption. Therefore we do not reject the null hypothesis. All of the models are acceptable.

$$H_0:\text{Normality assumption is not suspect}$$
$$H_1:\text{Normality assumption is suspect}$$

In addition, the `p-value` obtained from `Shapiro-Wilk normality test` shows that all of the model have an evidence to reject null hypothesis at $\alpha=0.05$. However, model 3 is most preferred becaused it shows very large `p-value` in `Shapiro-Wilk normality test`.

```{r}
# Cook's distance of the model of degree 2
corrosion_cd = cooks.distance(corrosion_model2)
idx1 = which(corrosion_cd > 4 / length(corrosion_cd))
sum(corrosion_cd > 4 / length(corrosion_cd))

# Cook's distance of the model of degree 3
corrosion_cd = cooks.distance(corrosion_model3)
sum(corrosion_cd > 4 / length(corrosion_cd))

# Cook's distance of the model of degree 4
corrosion_cd = cooks.distance(corrosion_model4)
idx2 = which(corrosion_cd > 4 / length(corrosion_cd))
sum(corrosion_cd > 4 / length(corrosion_cd))

```

It turns out that the model of degree 2 and degree 4 contain a highly influential observation. The index of this observations are `r idx1` and `r idx2` respectively.


***

## Exercise 5 (Diamonds)

The data set `diamonds` from the `ggplot2` package contains prices and characteristics of 54,000 diamonds. For this exercise, use `price` as the response variable $y$, and `carat` as the predictor $x$. Use `?diamonds` to learn more.

```{r, message = FALSE, warning = FALSE}
library(ggplot2)
```

**(a)** Fit a linear model with `price` as the response variable $y$, and `carat` as the predictor $x$. Return the summary information of this model.

```{r}
price_model = lm(price~carat, data=diamonds)
summary(price_model)
```


**(b)** Plot a scatterplot of price versus carat and add the line for the fitted model in part **(a)**. Using a fitted versus residuals plot and/or a Q-Q plot, comment on the diagnostics. 

```{r}
plot(diamonds$carat, diamonds$price, col="grey", xlab='carat', ylab='price', main='Relationship between carat and price')
abline(price_model, col='orange', lwd=3)

diagnostics(price_model, testit = FALSE)
```

Scatter plot shows that `constant variable assumption` is violated. Fitted vs residual plot shows also supports that `constant variable assumption is violated`. Finally, normal Q-Q plot shows that `normal distribution assumption` is violated.


**(c)** Seeing as the price stretches over several orders of magnitude, it seems reasonable to try a log transformation of the response. Fit a model with a logged response, plot a scatterplot of log-price versus carat and add the line for the fitted model, then use a fitted versus residuals plot and/or a Q-Q plot to comment on the diagnostics of the model.

```{r}
qplot(price, data = diamonds, bins = 30)
```

```{r}
price_model_exp = lm(log(price)~carat, data=diamonds)

plot(diamonds$carat, log(diamonds$price), col="grey", xlab='carat', ylab='log(price)', main='Relationship between carat and log(price)')
abline(price_model_exp, col='orange', lwd=3)

diagnostics(price_model_exp, testit = FALSE)

```

After log transformation on response, violation on both `same variance assumption` and `normal distribution assumption` have  mitigated. While the model now fits better, these two violations still exist. 

**(d)** Try adding log transformation of the predictor. Fit a model with a logged response and logged predictor, plot a scatterplot of log-price versus log-carat and add the line for the fitted model, then use a fitted versus residuals plot and/or a Q-Q plot to comment on the diagnostics of the model.

```{r}
price_model_expexp = lm(log(price)~log(carat), data=diamonds)

plot(log(diamonds$carat), log(diamonds$price), col="grey", xlab='log(carat)', ylab='log(price)', main='Relationship between log(carat) and log(price)')
abline(price_model_expexp, col="orange", lwd=3)

diagnostics(price_model_expexp, testit = FALSE)
```

After applying log transformation on predictor, the fitted line now almost perfectly fits the data. Plus, `fitted vs residuals` and `normal q-q plot` show that `contant variance assumption` and `normal distribution assumption` are mostly not suspect respectively. 

**(e)** Use the model from part **(d)** to predict the price (in dollars) of a 3-carat diamond. Construct a 99% prediction interval for the price (in dollars).

```{r}
(answer = exp(predict(price_model_expexp, interval="prediction", level=.99, newdata=data.frame(carat = exp(3)))))
```

The prediction interval is `r answer[2]` ~ `r answer[3]`
