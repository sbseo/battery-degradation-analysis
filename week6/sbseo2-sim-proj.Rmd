---
title: 'Week 6 - Simulation Project'
author: "STAT 420, Summer 2020, D. Unger"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
library(knitr)
opts_chunk$set(cache = TRUE, autodep = TRUE)
```

# Directions

This is an **individual** project. This is NOT like a homework assignment. This is NOT an assignment where collaboration is permissible. Discussion of question intent, coding problems/issues, and project administration may be discussed on the message board on a limited basis. However, sharing, copying, or providing any part of this project to another student is an infraction of the University’s rules on academic integrity. Any violation will be punished as severely as possible.

- Your project must be submitted through Coursera. You are required to upload one `.zip` file, named `yourNetID-sim-proj.zip`, which contains:
  + Your RMarkdown file which should be saved as `yourNetID-sim-proj.Rmd`.
  + `The result of knitting your RMarkdown file as `yourNetID-sim-proj.html`.
  + Any outside data provided as a `.csv` file. (In this case, `study_1.csv` and `study_2.csv`.)
- Your `.Rmd` file should be written such that, when stored in a folder with any data you are asked to import, it will knit properly without modification. If your `.zip` file is organized properly, this should not be an issue.
- Include your name and NetID in the final document, not only in your filenames.

This project consists of **three** simulation studies. Unlike a homework assignment, these "exercises" are not broken down into parts (e.g., a, b, c), and so your analysis will not be similarly partitioned. Instead, your document should be organized more like a true project report, and it should use the overall format:

- Simulation Study 1
- Simulation Study 2
- Simulation Study 3

Within each of the simulation studies, you should use the format:

- Introduction
- Methods
- Results
- Discussion

The **introduction** section should relay what you are attempting to accomplish. It should provide enough background to your work such that a reader would not need this directions document to understand what you are doing. (Basically, assume the reader is mostly familiar with the concepts from the course, but not this project.)

The **methods** section should contain the majority of your “work.” This section will contain the bulk of the `R` code that is used to generate the results. Your `R` code is not expected to be perfect idiomatic `R`, but it is expected to be understood by a reader without too much effort. Use RMarkdown and code comments to your advantage to explain your code if needed.

The **results** section should contain numerical or graphical summaries of your results as they pertain to the goal of each study.

The **discussion** section should contain discussion of your results. The discussion section should contain discussion of your results. Potential topics for discussion are suggested at the end of each simulation study section, but they are not meant to be an exhaustive list. These simulation studies are meant to be explorations into the principles of statistical modeling, so do not limit your responses to short, closed form answers as you do in homework assignments. Use the potential discussion questions as a starting point for your response.

- Your resulting `.html` file will be considered a self-contained “report,” which is the material that will determine the majority of your grade. Be sure to visibly include all `R` code and output that is *relevant*. (You should not include irrelevant code you tried that resulted in error or did not answer the question correctly.)
- Grading will be based on a combination of completing the required tasks, discussion of results, `R` usage, RMarkdown usage, and neatness and organization. For full details see the provided rubric.
- At the beginning of *each* of the three simulation studies, set a seed equal to your birthday, as is done on homework. (It should be the first code run for each study.) These should be the only three times you set a seed.

```{r}
birthday = 19891210
set.seed(birthday)
```

# Simulation Study 1: Significance of Regression

In this simulation study we will investigate the significance of regression test. We will simulate from two different models:

1. The **"significant"** model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 3$,
- $\beta_1 = 1$,
- $\beta_2 = 1$,
- $\beta_3 = 1$.


2. The **"non-significant"** model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 3$,
- $\beta_1 = 0$,
- $\beta_2 = 0$,
- $\beta_3 = 0$.

For both, we will consider a sample size of $25$ and three possible levels of noise. That is, three values of $\sigma$.

- $n = 25$
- $\sigma \in (1, 5, 10)$

Use simulation to obtain an empirical distribution for each of the following values, for each of the three values of $\sigma$, for both models.

- The **$F$ statistic** for the significance of regression test.
- The **p-value** for the significance of regression test
- **$R^2$**

For each model and $\sigma$ combination, use $2000$ simulations. For each simulation, fit a regression model of the same form used to perform the simulation.

Use the data found in [`study_1.csv`](study_1.csv) for the values of the predictors. These should be kept constant for the entirety of this study. The `y` values in this data are a blank placeholder.

Done correctly, you will have simulated the `y` vector $2 (models)×3 (sigmas)×2000 (sims)=12000$ times.

Potential discussions:

- Do we know the true distribution of any of these values?
- How do the empirical distributions from the simulations compare to the true distributions? (You could consider adding a curve for the true distributions if you know them.)
- How are each of the $F$ statistic, the p-value, and $R^2$ related to $\sigma$? Are any of those relationships the same for the significant and non-significant models?

Additional things to consider:

- Organize the plots in a grid for easy comparison.


### Introduction

In simulation study 1, we simulate two different models and conduct significance of regression test. At the end of the question, we will visualize the simulation result by plots and tables.

### Methods

$$H_0 : \beta_1 = \beta_2 = \beta_3 = 0 \text{ vs } H_1: \text{not } H_0$$

Signifance of regression test will be used to perform the simulation. In significance of regression test, null hypothesis assumes all of coefficients are zero, while alternative hypothesis assumes at least one of the coefficients is not zero. For individual parameter test, we can simply analyze p-values of fitted coefficients. To test the entire model given the every existing parameter, F-statistic is an useful measure. We also check $R^2$ while it won't guarantee the optimal result in multiple linear regression model.


```{r}
library(readr)
data_study_1 = read_csv("study_1.csv")
df = data.frame(y = data_study_1$y,
                x1 = data_study_1$x1,
                x2 = data_study_1$x2,
                x3 = data_study_1$x3)
```

```{r}
# Significant model
s_beta_0 = 3
s_beta_1 = 1
s_beta_2 = 1
s_beta_3 = 1

# Non-significant model
ns_beta_0 = 3
ns_beta_1 = 0
ns_beta_2 = 0
ns_beta_3 = 0

# Sample size and noise
n = 25
sigma = c(1, 5, 10)
```

```{r}
sim_slr = function(df, beta_0, beta_1, beta_2, beta_3, sigma) {
  epsilon = rnorm(n, mean = 0, sd = sigma)
  df$y = beta_0 + beta_1*df$x1 + beta_2*df$x2 + beta_3*df$x3 + epsilon
  df
}
```


```{r}
library(broom)

# 1 
# Significant model, sigma: 1
pvals_sig1_s = rep(0, n)
fstats_sig1_s = rep(0, n)
rsquares_sig1_s = rep(0, n)
beta_hat_0_sig1_s = rep(0, n)
beta_hat_1_sig1_s = rep(0, n)
beta_hat_2_sig1_s = rep(0, n)
beta_hat_3_sig1_s = rep(0, n)

for (i in 1:2000) {
  sim_data = sim_slr(df, s_beta_0, s_beta_1, s_beta_2, s_beta_3, sigma=sigma[1])
  model_sig1_s = lm(y~., data=sim_data)
  sim_data  
  fstats_sig1_s[i] = summary(model_sig1_s)$fstatistic[1]
  pvals_sig1_s[i] = glance(model_sig1_s)$p.value
  rsquares_sig1_s[i] = summary(model_sig1_s)$r.squared
  beta_hat_0_sig1_s[i] = summary(model_sig1_s)$coefficients["(Intercept)","Estimate"]
  beta_hat_1_sig1_s[i] = summary(model_sig1_s)$coefficients["x1","Estimate"]
  beta_hat_2_sig1_s[i] = summary(model_sig1_s)$coefficients["x2","Estimate"]
  beta_hat_3_sig1_s[i] = summary(model_sig1_s)$coefficients["x3","Estimate"]
}

# 2
pvals_sig5_s = rep(0, n)
fstats_sig5_s = rep(0, n)
rsquares_sig5_s = rep(0, n)
beta_hat_0_sig5_s = rep(0, n)
beta_hat_1_sig5_s = rep(0, n)
beta_hat_2_sig5_s = rep(0, n)
beta_hat_3_sig5_s = rep(0, n)

for (i in 1:2000) {
  sim_data = sim_slr(df, s_beta_0, s_beta_1, s_beta_2, s_beta_3, sigma=sigma[2])
  model_sig5_s = lm(y~., data=sim_data)
    
  fstats_sig5_s[i] = summary(model_sig5_s)$fstatistic[1]
  pvals_sig5_s[i] = glance(model_sig5_s)$p.value
  rsquares_sig5_s[i] = summary(model_sig5_s)$r.squared
  beta_hat_0_sig5_s[i] = summary(model_sig5_s)$coefficients["(Intercept)","Estimate"]
  beta_hat_1_sig5_s[i] = summary(model_sig5_s)$coefficients["x1","Estimate"]
  beta_hat_2_sig5_s[i] = summary(model_sig5_s)$coefficients["x2","Estimate"]
  beta_hat_3_sig5_s[i] = summary(model_sig5_s)$coefficients["x3","Estimate"]
}

# 3
pvals_sig10_s = rep(0, n)
fstats_sig10_s = rep(0, n)
rsquares_sig10_s = rep(0, n)
beta_hat_0_sig10_s = rep(0, n)
beta_hat_1_sig10_s = rep(0, n)
beta_hat_2_sig10_s = rep(0, n)
beta_hat_3_sig10_s = rep(0, n)

for (i in 1:2000) {
  sim_data = sim_slr(df, s_beta_0, s_beta_1, s_beta_2, s_beta_3, sigma=sigma[3])
  model_sig10_s = lm(y~., data=sim_data)
    
  fstats_sig10_s[i] = summary(model_sig10_s)$fstatistic[1]
  pvals_sig10_s[i] = glance(model_sig10_s)$p.value
  rsquares_sig10_s[i] = summary(model_sig10_s)$r.squared
  beta_hat_0_sig10_s[i] = summary(model_sig10_s)$coefficients["(Intercept)","Estimate"]
  beta_hat_1_sig10_s[i] = summary(model_sig10_s)$coefficients["x1","Estimate"]
  beta_hat_2_sig10_s[i] = summary(model_sig10_s)$coefficients["x2","Estimate"]
  beta_hat_3_sig10_s[i] = summary(model_sig10_s)$coefficients["x3","Estimate"]
  }

# Non significant model, sigma: 1
pvals_sig1_ns = rep(0, n)
fstats_sig1_ns = rep(0, n)
rsquares_sig1_ns = rep(0, n)
beta_hat_0_sig1_ns = rep(0, n)
beta_hat_1_sig1_ns = rep(0, n)
beta_hat_2_sig1_ns = rep(0, n)
beta_hat_3_sig1_ns = rep(0, n)

for (i in 1:2000) {
  sim_data = sim_slr(df, ns_beta_0, ns_beta_1, ns_beta_2, ns_beta_3, sigma=sigma[1])
  model_sig1_ns = lm(y~., data=sim_data)
    
  fstats_sig1_ns[i] = summary(model_sig1_ns)$fstatistic[1]
  pvals_sig1_ns[i] = glance(model_sig1_ns)$p.value
  rsquares_sig1_ns[i] = summary(model_sig1_ns)$r.squared
  beta_hat_0_sig1_ns[i] = summary(model_sig1_ns)$coefficients["(Intercept)","Estimate"]
  beta_hat_1_sig1_ns[i] = summary(model_sig1_ns)$coefficients["x1","Estimate"]
  beta_hat_2_sig1_ns[i] = summary(model_sig1_ns)$coefficients["x2","Estimate"]
  beta_hat_3_sig1_ns[i] = summary(model_sig1_ns)$coefficients["x3","Estimate"]
}

# 2
pvals_sig5_ns = rep(0, n)
fstats_sig5_ns = rep(0, n)
rsquares_sig5_ns = rep(0, n)
beta_hat_0_sig5_ns = rep(0, n)
beta_hat_1_sig5_ns = rep(0, n)
beta_hat_2_sig5_ns = rep(0, n)
beta_hat_3_sig5_ns = rep(0, n)

for (i in 1:2000) {
  sim_data = sim_slr(df, ns_beta_0, ns_beta_1, ns_beta_2, ns_beta_3, sigma=sigma[2])
  model_sig5_ns = lm(y~., data=sim_data)
    
  fstats_sig5_ns[i] = summary(model_sig5_ns)$fstatistic[1]
  pvals_sig5_ns[i] = glance(model_sig5_ns)$p.value
  rsquares_sig5_ns[i] = summary(model_sig5_ns)$r.squared
  beta_hat_0_sig5_ns[i] = summary(model_sig5_ns)$coefficients["(Intercept)","Estimate"]
  beta_hat_1_sig5_ns[i] = summary(model_sig5_ns)$coefficients["x1","Estimate"]
  beta_hat_2_sig5_ns[i] = summary(model_sig5_ns)$coefficients["x2","Estimate"]
  beta_hat_3_sig5_ns[i] = summary(model_sig5_ns)$coefficients["x3","Estimate"]
}

# 3
pvals_sig10_ns = rep(0, n)
fstats_sig10_ns = rep(0, n)
rsquares_sig10_ns = rep(0, n)
beta_hat_0_sig10_ns = rep(0, n)
beta_hat_1_sig10_ns = rep(0, n)
beta_hat_2_sig10_ns = rep(0, n)
beta_hat_3_sig10_ns = rep(0, n)

for (i in 1:2000) {
  sim_data = sim_slr(df, ns_beta_0, ns_beta_1, ns_beta_2, ns_beta_3, sigma=sigma[3])
  model_sig10_ns = lm(y~., data=sim_data)
    
  fstats_sig10_ns[i] = summary(model_sig10_ns)$fstatistic[1]
  pvals_sig10_ns[i] = glance(model_sig10_ns)$p.value
  rsquares_sig10_ns[i] = summary(model_sig10_ns)$r.squared
  beta_hat_0_sig10_ns[i] = summary(model_sig10_ns)$coefficients["(Intercept)","Estimate"]
  beta_hat_1_sig10_ns[i] = summary(model_sig10_ns)$coefficients["x1","Estimate"]
  beta_hat_2_sig10_ns[i] = summary(model_sig10_ns)$coefficients["x2","Estimate"]
  beta_hat_3_sig10_ns[i] = summary(model_sig10_ns)$coefficients["x3","Estimate"]
  }

```


### Results

The table below shows the proportion of `f-statistic` that is less than $\alpha$ when simulated for 2000 times.The column name `0.01`, `0.05`, and `0.10` refers to the $\alpha$. `_s` and `_ns` in row mean significant model and non significant model respectively. `sig1`, `sig5`, and `sig10` mean the $\sigma$ which is used for noise within the model. 

```{r echo=FALSE}
df = data.frame(sig1_s = c(mean(fstats_sig1_s < 0.01),
                            mean(fstats_sig1_s < 0.05),
                            mean(fstats_sig1_s < 0.10)),
                sig1_ns = c(mean(fstats_sig1_ns < 0.01),
                            mean(fstats_sig1_ns < 0.05),
                            mean(fstats_sig1_ns < 0.10)),
                sig5_s = c(mean(fstats_sig5_s < 0.01),
                            mean(fstats_sig5_s < 0.05),
                            mean(fstats_sig5_s < 0.10)),
                sig5_ns = c(mean(fstats_sig5_ns < 0.01),
                            mean(fstats_sig5_ns < 0.05),
                            mean(fstats_sig5_ns < 0.10)),
                sig10_s = c(mean(fstats_sig10_s < 0.01),
                            mean(fstats_sig10_s < 0.05),
                            mean(fstats_sig10_s < 0.10)),
                sig10_ns = c(mean(fstats_sig10_ns < 0.01),
                            mean(fstats_sig10_ns < 0.05),
                            mean(fstats_sig10_ns < 0.10)))

kable(t(df), col.names = c('0.01','0.05','0.10'))
# sigma 
```

The table below shows the proportion of `p-values` less than $\alpha$ when simulated for 2000 times.

```{r echo=FALSE}
df = data.frame(sig1_s = c(mean(pvals_sig1_s < 0.01),
                            mean(pvals_sig1_s < 0.05),
                            mean(pvals_sig1_s < 0.10)),
                sig1_ns = c(mean(pvals_sig1_ns < 0.01),
                            mean(pvals_sig1_ns < 0.05),
                            mean(pvals_sig1_ns < 0.10)),
                sig5_s = c(mean(pvals_sig5_s < 0.01),
                            mean(pvals_sig5_s < 0.05),
                            mean(pvals_sig5_s < 0.10)),
                sig5_ns = c(mean(pvals_sig5_ns < 0.01),
                            mean(pvals_sig5_ns < 0.05),
                            mean(pvals_sig5_ns < 0.10)),
                sig10_s = c(mean(pvals_sig10_s < 0.01),
                            mean(pvals_sig10_s < 0.05),
                            mean(pvals_sig10_s < 0.10)),
                sig10_ns = c(mean(pvals_sig10_ns < 0.01),
                            mean(pvals_sig10_ns < 0.05),
                            mean(pvals_sig10_ns < 0.10)))

kable(t(df), col.names = c('0.01','0.05','0.10'))
```

The table below shows $R^2$ measure obtained from signifcant and non significant model when $\sigma$ is  1, 5, and 10. 

```{r echo=FALSE}
df = data.frame(sig1 = c(summary(model_sig1_s)$r.squared, summary(model_sig1_ns)$r.squared),
                sig5 = c(summary(model_sig5_s)$r.squared, summary(model_sig5_ns)$r.squared),
                sig10 = c(summary(model_sig10_s)$r.squared, summary(model_sig10_ns)$r.squared))

kable(t(df), col.names = c("Significant", "Non significant"))

```



### Discussion

In f-test, the result shows that `sig1_s` is almost certain to be a significant model. There are almost no proportion less than 0.01, 0.05, and 0.10$\%$. Therefore, null hypothesis is rejected. The overall `f-statistic` increased as $\sigma$ increased. Considering `sig1_s` and `sig10_s` really are signicant model, the result indicates that the larger sigma may cause a larger error rate.

The result of t-test is also similar to f-test. `sig1_s` was tested to be a signifcant model. However, confidence interval was decreased as $\sigma$ has increased.

$R^2$ means the proportion of the regression explained by the model. Therefore, is easy to infer that significant model would have higher measure than non significant model. As expected, the result showed that significant model with lower $\sigma$ obtained larger $R^2$ measure. 

Overall, both significant and non-significant model share the same relationship with $\sigma$, while there are few exceptions when $\sigma$ is a large number. 



The graphs below show the result of one parameter test for every coefficient. The true distributions of significant model are known as the following. 
$$\hat{\beta_0} \sim N(\beta_0=`r s_beta_0`, \sigma^2 = `r summary(model_sig5_ns)$coefficients['(Intercept)', 2]`^2)$$
$$\hat{\beta_1} \sim N(\beta_1=`r s_beta_1`, \sigma^2 = `r summary(model_sig5_ns)$coefficients['x1', 2]`^2)$$
$$\hat{\beta_2} \sim N(\beta_2=`r s_beta_2`, \sigma^2 = `r summary(model_sig5_ns)$coefficients['x2', 2]`^2)$$
$$\hat{\beta_3} \sim N(\beta_3=`r s_beta_3`, \sigma^2 = `r summary(model_sig5_ns)$coefficients['x3', 2]`^2)$$
Likewise, the graphs below show that the empirical distribution truely follows the true distribution.

```{r echo=FALSE} 
par(mfrow = c(3,2))
hist(beta_hat_0_sig1_s  , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[0])))
curve(dnorm(x, mean = s_beta_0, sd = summary(model_sig1_s)$coefficients['(Intercept)', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_0_sig5_s  , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[0])))
curve(dnorm(x, mean = s_beta_0, sd = summary(model_sig5_s)$coefficients['(Intercept)', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_0_sig10_s , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[0])))
curve(dnorm(x, mean = s_beta_0, sd = summary(model_sig10_s)$coefficients['(Intercept)', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_0_sig1_ns , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[0])))
curve(dnorm(x, mean = ns_beta_0, sd = summary(model_sig1_ns)$coefficients['(Intercept)', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_0_sig5_ns , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[0])))
curve(dnorm(x, mean = ns_beta_0, sd = summary(model_sig5_ns)$coefficients['(Intercept)', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_0_sig10_ns, prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[0])))
curve(dnorm(x, mean = ns_beta_0, sd = summary(model_sig10_ns)$coefficients['(Intercept)', 2]),
      col = "darkorange", add = TRUE, lwd = 3)
```
```{r echo=FALSE} 
par(mfrow = c(3,2))
hist(beta_hat_1_sig1_s  , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[1])))
curve(dnorm(x, mean = s_beta_1, sd = summary(model_sig1_s)$coefficients['x1', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_1_sig5_s  , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[1])))
curve(dnorm(x, mean = s_beta_1, sd = summary(model_sig5_s)$coefficients['x1', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_1_sig10_s , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[1])))
curve(dnorm(x, mean = s_beta_1, sd = summary(model_sig10_s)$coefficients['x1', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_1_sig1_ns , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[1])))
curve(dnorm(x, mean = ns_beta_1, sd = summary(model_sig1_ns)$coefficients['x1', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_1_sig5_ns , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[1])))
curve(dnorm(x, mean = ns_beta_1, sd = summary(model_sig5_ns)$coefficients['x1', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_1_sig10_ns, prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[1])))
curve(dnorm(x, mean = ns_beta_1, sd = summary(model_sig10_ns)$coefficients['x1', 2]),
      col = "darkorange", add = TRUE, lwd = 3)
```
```{r echo=FALSE}
par(mfrow = c(3,2))
hist(beta_hat_2_sig1_s  , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[2])))
curve(dnorm(x, mean = s_beta_2, sd = summary(model_sig1_s)$coefficients['x2', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_2_sig5_s  , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[2])))
curve(dnorm(x, mean = s_beta_2, sd = summary(model_sig5_s)$coefficients['x2', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_2_sig10_s , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[2])))
curve(dnorm(x, mean = s_beta_2, sd = summary(model_sig10_s)$coefficients['x2', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_2_sig1_ns , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[2])))
curve(dnorm(x, mean = ns_beta_2, sd = summary(model_sig1_ns)$coefficients['x2', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_2_sig5_ns , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[2])))
curve(dnorm(x, mean = ns_beta_2, sd = summary(model_sig5_ns)$coefficients['x2', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_2_sig10_ns, prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[2])))
curve(dnorm(x, mean = ns_beta_2, sd = summary(model_sig10_ns)$coefficients['x2', 2]),
      col = "darkorange", add = TRUE, lwd = 3)
```
```{r echo=FALSE}
par(mfrow = c(3,2))
hist(beta_hat_3_sig1_s  , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[3])))
curve(dnorm(x, mean = s_beta_3, sd = summary(model_sig1_s)$coefficients['x3', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_3_sig5_s  , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[3])))
curve(dnorm(x, mean = s_beta_3, sd = summary(model_sig5_s)$coefficients['x3', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_3_sig10_s , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[3])))
curve(dnorm(x, mean = s_beta_3, sd = summary(model_sig10_s)$coefficients['x3', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_3_sig1_ns , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[3])))
curve(dnorm(x, mean = ns_beta_3, sd = summary(model_sig1_ns)$coefficients['x3', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_3_sig5_ns , prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[3])))
curve(dnorm(x, mean = ns_beta_3, sd = summary(model_sig5_ns)$coefficients['x3', 2]),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_3_sig10_ns, prob = TRUE, breaks = 20, border='dodgerblue', xlab = expression(hat(beta[3])))
curve(dnorm(x, mean = ns_beta_3, sd = summary(model_sig10_ns)$coefficients['x3', 2]),
      col = "darkorange", add = TRUE, lwd = 3)
```

For future work, confidence interval and prediction interval for mean response can be disscussed.


# Simulation Study 2: Using RMSE for Selection?

In homework we saw how Test RMSE can be used to select the “best” model. In this simulation study we will investigate how well this procedure works. Since splitting the data is random, we don’t expect it to work correctly each time. We could get unlucky. But averaged over many attempts, we should expect it to select the appropriate model.

We will simulate from the model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \beta_5 x_{i5} + \beta_6 x_{i6} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 0$,
- $\beta_1 = 3$,
- $\beta_2 = -4$,
- $\beta_3 = 1.6$,
- $\beta_4 = -1.1$,
- $\beta_5 = 0.7$,
- $\beta_6 = 0.5$.

We will consider a sample size of $500$ and three possible levels of noise. That is, three values of $\sigma$.

- $n = 500$
- $\sigma \in (1, 2, 4)$

Use the data found in [`study_2.csv`](study_2.csv) for the values of the predictors. These should be kept constant for the entirety of this study. The `y` values in this data are a blank placeholder.

Each time you simulate the data, randomly split the data into train and test sets of equal sizes (250 observations for training, 250 observations for testing).

For each, fit **nine** models, with forms:

- `y ~ x1`
- `y ~ x1 + x2`
- `y ~ x1 + x2 + x3`
- `y ~ x1 + x2 + x3 + x4`
- `y ~ x1 + x2 + x3 + x4 + x5`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6`, the correct form of the model as noted above
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9`

For each model, calculate Train and Test RMSE.

\[
\text{RMSE}(\text{model, data}) = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}
\]

Repeat this process with $1000$ simulations for each of the $3$ values of $\sigma$. For each value of $\sigma$, create a plot that shows how average Train RMSE and average Test RMSE changes as a function of model size. Also show the number of times the model of each size was chosen for each value of $\sigma$.

Done correctly, you will have simulated the $y$ vector $3×1000=3000$ times. You will have fit $9×3×1000=27000$ models. A minimal result would use $3$ plots. Additional plots may also be useful.

Potential discussions:

- Does the method **always** select the correct model? On average, does is select the correct model?
- How does the level of noise affect the results?

## Introduction
Simulation study 2 examines whether RSME metric succesfully selects the best model among different multiple linear regression models.

## Methods
We split training and testing set following the ratio of $1:1$. Then 
we collect RSME results from every different model. Based on this metric, the best model will have the lowest measure.

```{r}
birthday = 19891210
set.seed(birthday)
```

```{r}
beta_0 = 0
beta_1 = 3
beta_2 = -4
beta_3 = 1.6
beta_4 = -1.1
beta_5 = 0.7
beta_6 = 0.5
n = 500
sigma = c(1,2,4)
```

```{r message=FALSE}
study2_data = read_csv("study_2.csv")
```
```{r}
RMSE = function(criterion, model){
  
  y = study2_data[criterion, ]$y
  y_hat = predict(model, newdata=data.frame(x1=study2_data[criterion, ]$x1,
                                            x2=study2_data[criterion, ]$x2,
                                            x3=study2_data[criterion, ]$x3,
                                            x4=study2_data[criterion, ]$x4,
                                            x5=study2_data[criterion, ]$x5,
                                            x6=study2_data[criterion, ]$x6,
                                            x7=study2_data[criterion, ]$x7,
                                            x8=study2_data[criterion, ]$x8,
                                            x9=study2_data[criterion, ]$x9))
  sqrt(sum((y - y_hat)^2) / length(y))
}
```

```{r}
trn_ml1 = rep(0, 3000)
tst_ml1 = rep(0, 3000)
trn_ml2 = rep(0, 3000)
tst_ml2 = rep(0, 3000)
trn_ml3 = rep(0, 3000)
tst_ml3 = rep(0, 3000)
trn_ml4 = rep(0, 3000)
tst_ml4 = rep(0, 3000)
trn_ml5 = rep(0, 3000)
tst_ml5 = rep(0, 3000)
trn_ml6 = rep(0, 3000)
tst_ml6 = rep(0, 3000)
trn_ml7 = rep(0, 3000)
tst_ml7 = rep(0, 3000)
trn_ml8 = rep(0, 3000)
tst_ml8 = rep(0, 3000)
trn_ml9 = rep(0, 3000)
tst_ml9 = rep(0, 3000)

for (i in 1:1000){
  trn_idx = sample(1:nrow(study2_data), 250)
  trn_idx = sort(trn_idx)
  trn_data = study2_data[trn_idx,]
  val_data = study2_data[-trn_idx,]
  val_idx = seq(1:500)[-trn_idx]
  
  epsilon = rnorm(n/2, 0, 1)
  study2_data$y = beta_0 + beta_1*study2_data$x1+ beta_2*study2_data$x2 + beta_3*study2_data$x3 + beta_4*study2_data$x4 + beta_5*study2_data$x5 + beta_6*study2_data$x6 + epsilon
  
  model1 = lm(y ~ x1, data=study2_data[trn_idx, ])
  model2 = lm(y ~ x1+x2, data=study2_data[trn_idx, ])
  model3 = lm(y ~ x1+x2+x3, data=study2_data[trn_idx, ])
  model4 = lm(y ~ x1+x2+x3+x4, data=study2_data[trn_idx, ])
  model5 = lm(y ~ x1+x2+x3+x4+x5, data=study2_data[trn_idx, ])
  model6 = lm(y ~ x1+x2+x3+x4+x5+x6, data=study2_data[trn_idx, ])
  model7 = lm(y ~ x1+x2+x3+x4+x5+x6+x7, data=study2_data[trn_idx, ])
  model8 = lm(y ~ x1+x2+x3+x4+x5+x6+x7+x8, data=study2_data[trn_idx, ])
  model9 = lm(y ~ x1+x2+x3+x4+x5+x6+x7+x8+x9, data=study2_data[trn_idx, ])
  
  # Model1
  trn_ml1[i] = RMSE(trn_idx, model1)
  tst_ml1[i] = RMSE(val_idx, model1)
  #Model2
  trn_ml2[i] = RMSE(trn_idx, model2)
  tst_ml2[i] = RMSE(val_idx, model2)
  #Model3
  trn_ml3[i] = RMSE(trn_idx, model3)
  tst_ml3[i] = RMSE(val_idx, model3)
  #Model4
  trn_ml4[i] = RMSE(trn_idx, model4)
  tst_ml4[i] = RMSE(val_idx, model4)
  #Model5
  trn_ml5[i] = RMSE(trn_idx, model5)
  tst_ml5[i] = RMSE(val_idx, model5)
  #Model6
  trn_ml6[i] = RMSE(trn_idx, model6)
  tst_ml6[i] = RMSE(val_idx, model6)
  #Model7
  trn_ml7[i] = RMSE(trn_idx, model7)
  tst_ml7[i] = RMSE(val_idx, model7)
  #Model8
  trn_ml8[i] = RMSE(trn_idx, model8)
  tst_ml8[i] = RMSE(val_idx, model8)
  #Model9
  trn_ml9[i] = RMSE(trn_idx, model9)
  tst_ml9[i] = RMSE(val_idx, model9)
}

```

```{r}
for (i in 1001:2000){
  trn_idx = sample(1:nrow(study2_data), 250)
  trn_data = study2_data[trn_idx,]
  val_data = study2_data[-trn_idx,]
  val_idx = seq(1:500)[-trn_idx]
  
  epsilon = rnorm(n/2, 0, 3)
    study2_data$y = beta_0 + beta_1*study2_data$x1+ beta_2*study2_data$x2 + beta_3*study2_data$x3 + beta_4*study2_data$x4 + beta_5*study2_data$x5 + beta_6*study2_data$x6 + epsilon
  
  model1 = lm(y ~ x1, data=study2_data[trn_idx, ])
  model2 = lm(y ~ x1+x2, data=study2_data[trn_idx, ])
  model3 = lm(y ~ x1+x2+x3, data=study2_data[trn_idx, ])
  model4 = lm(y ~ x1+x2+x3+x4, data=study2_data[trn_idx, ])
  model5 = lm(y ~ x1+x2+x3+x4+x5, data=study2_data[trn_idx, ])
  model6 = lm(y ~ x1+x2+x3+x4+x5+x6, data=study2_data[trn_idx, ])
  model7 = lm(y ~ x1+x2+x3+x4+x5+x6+x7, data=study2_data[trn_idx, ])
  model8 = lm(y ~ x1+x2+x3+x4+x5+x6+x7+x8, data=study2_data[trn_idx, ])
  model9 = lm(y ~ x1+x2+x3+x4+x5+x6+x7+x8+x9, data=study2_data[trn_idx, ])
  
  # Model1
  trn_ml1[i] = RMSE(trn_idx, model1)
  tst_ml1[i] = RMSE(val_idx, model1)
  #Model2
  trn_ml2[i] = RMSE(trn_idx, model2)
  tst_ml2[i] = RMSE(val_idx, model2)
  #Model3
  trn_ml3[i] = RMSE(trn_idx, model3)
  tst_ml3[i] = RMSE(val_idx, model3)
  #Model4
  trn_ml4[i] = RMSE(trn_idx, model4)
  tst_ml4[i] = RMSE(val_idx, model4)
  #Model5
  trn_ml5[i] = RMSE(trn_idx, model5)
  tst_ml5[i] = RMSE(val_idx, model5)
  #Model6
  trn_ml6[i] = RMSE(trn_idx, model6)
  tst_ml6[i] = RMSE(val_idx, model6)
  #Model7
  trn_ml7[i] = RMSE(trn_idx, model7)
  tst_ml7[i] = RMSE(val_idx, model7)
  #Model8
  trn_ml8[i] = RMSE(trn_idx, model8)
  tst_ml8[i] = RMSE(val_idx, model8)
  #Model9
  trn_ml9[i] = RMSE(trn_idx, model9)
  tst_ml9[i] = RMSE(val_idx, model9)
}
for (i in 2001:3000){
  trn_idx = sample(1:nrow(study2_data), 250)
  trn_data = study2_data[trn_idx,]
  val_data = study2_data[-trn_idx,]
  val_idx = seq(1:500)[-trn_idx]
  
  epsilon = rnorm(n/2, 0, 5)
  study2_data$y = beta_0 + beta_1*study2_data$x1+ beta_2*study2_data$x2 + beta_3*study2_data$x3 + beta_4*study2_data$x4 + beta_5*study2_data$x5 + beta_6*study2_data$x6 + epsilon
  
  model1 = lm(y ~ x1, data=study2_data[trn_idx, ])
  model2 = lm(y ~ x1+x2, data=study2_data[trn_idx, ])
  model3 = lm(y ~ x1+x2+x3, data=study2_data[trn_idx, ])
  model4 = lm(y ~ x1+x2+x3+x4, data=study2_data[trn_idx, ])
  model5 = lm(y ~ x1+x2+x3+x4+x5, data=study2_data[trn_idx, ])
  model6 = lm(y ~ x1+x2+x3+x4+x5+x6, data=study2_data[trn_idx, ])
  model7 = lm(y ~ x1+x2+x3+x4+x5+x6+x7, data=study2_data[trn_idx, ])
  model8 = lm(y ~ x1+x2+x3+x4+x5+x6+x7+x8, data=study2_data[trn_idx, ])
  model9 = lm(y ~ x1+x2+x3+x4+x5+x6+x7+x8+x9, data=study2_data[trn_idx, ])
  
  # Model1
  trn_ml1[i] = RMSE(trn_idx, model1)
  tst_ml1[i] = RMSE(val_idx, model1)
  #Model2
  trn_ml2[i] = RMSE(trn_idx, model2)
  tst_ml2[i] = RMSE(val_idx, model2)
  #Model3
  trn_ml3[i] = RMSE(trn_idx, model3)
  tst_ml3[i] = RMSE(val_idx, model3)
  #Model4
  trn_ml4[i] = RMSE(trn_idx, model4)
  tst_ml4[i] = RMSE(val_idx, model4)
  #Model5
  trn_ml5[i] = RMSE(trn_idx, model5)
  tst_ml5[i] = RMSE(val_idx, model5)
  #Model6
  trn_ml6[i] = RMSE(trn_idx, model6)
  tst_ml6[i] = RMSE(val_idx, model6)
  #Model7
  trn_ml7[i] = RMSE(trn_idx, model7)
  tst_ml7[i] = RMSE(val_idx, model7)
  #Model8
  trn_ml8[i] = RMSE(trn_idx, model8)
  tst_ml8[i] = RMSE(val_idx, model8)
  #Model9
  trn_ml9[i] = RMSE(trn_idx, model9)
  tst_ml9[i] = RMSE(val_idx, model9)
}
```

## Results

The first plot and table shows the RSME result when $\sigma = 1$. The integer in x-axis refers to the number of predictors used for the model.

```{r echo=FALSE}
df = data.frame(Model1 = c(mean(trn_ml1[1:1000]), mean(tst_ml1[1:1000])),
                Model2 = c(mean(trn_ml2[1:1000]), mean(tst_ml2[1:1000])),
                Model3 = c(mean(trn_ml3[1:1000]), mean(tst_ml3[1:1000])),
                Model4 = c(mean(trn_ml4[1:1000]), mean(tst_ml4[1:1000])),
                Model5 = c(mean(trn_ml5[1:1000]), mean(tst_ml5[1:1000])),
                Model6 = c(mean(trn_ml6[1:1000]), mean(tst_ml6[1:1000])),
                Model7 = c(mean(trn_ml7[1:1000]), mean(tst_ml7[1:1000])),
                Model8 = c(mean(trn_ml8[1:1000]), mean(tst_ml8[1:1000])),
                Model9 = c(mean(trn_ml9[1:1000]), mean(tst_ml9[1:1000])))


bplot = barplot(
  matrix(c(df$Model1, df$Model2, df$Model3, df$Model4, df$Model5, df$Model6, df$Model7, df$Model8, df$Model9), nrow = 2, ncol = 9),
  beside = TRUE,
  xlab='Model variations',
  names.arg=c(seq(1:9)),
  ylab='RSME',
  legend.text = c('train data', 'test data'),
  main="RSME where sigma = 1",
  col=c("lightblue","orange")
)

kable(t(df), col.names = c("Training", "Testing"))
```

Below is the result where $\sigma=3$.

```{r echo=FALSE}
df = data.frame(Model1 = c(mean(trn_ml1[1001:2000]), mean(tst_ml1[1001:2000])),
                Model2 = c(mean(trn_ml2[1001:2000]), mean(tst_ml2[1001:2000])),
                Model3 = c(mean(trn_ml3[1001:2000]), mean(tst_ml3[1001:2000])),
                Model4 = c(mean(trn_ml4[1001:2000]), mean(tst_ml4[1001:2000])),
                Model5 = c(mean(trn_ml5[1001:2000]), mean(tst_ml5[1001:2000])),
                Model6 = c(mean(trn_ml6[1001:2000]), mean(tst_ml6[1001:2000])),
                Model7 = c(mean(trn_ml7[1001:2000]), mean(tst_ml7[1001:2000])),
                Model8 = c(mean(trn_ml8[1001:2000]), mean(tst_ml8[1001:2000])),
                Model9 = c(mean(trn_ml9[1001:2000]), mean(tst_ml9[1001:2000])))

barplot(
  matrix(c(df$Model1, df$Model2, df$Model3, df$Model4, df$Model5, df$Model6, df$Model7, df$Model8, df$Model9), nrow = 2, ncol = 9),
  beside = TRUE,
  xlab='Model variations',
  names.arg=c(seq(1:9)),
  ylab='RSME',
  legend.text = c('train data', 'test data'),
  main="RSME where sigma = 3",
  col=c("lightblue","orange")
)
kable(t(df), col.names = c("Training", "Testing"))
```

Below is the result where $\sigma=5$.

```{r echo=FALSE}
df = data.frame(Model1 = c(mean(trn_ml1[2001:3000]), mean(tst_ml1[2001:3000])),
                Model2 = c(mean(trn_ml2[2001:3000]), mean(tst_ml2[2001:3000])),
                Model3 = c(mean(trn_ml3[2001:3000]), mean(tst_ml3[2001:3000])),
                Model4 = c(mean(trn_ml4[2001:3000]), mean(tst_ml4[2001:3000])),
                Model5 = c(mean(trn_ml5[2001:3000]), mean(tst_ml5[2001:3000])),
                Model6 = c(mean(trn_ml6[2001:3000]), mean(tst_ml6[2001:3000])),
                Model7 = c(mean(trn_ml7[2001:3000]), mean(tst_ml7[2001:3000])),
                Model8 = c(mean(trn_ml8[2001:3000]), mean(tst_ml8[2001:3000])),
                Model9 = c(mean(trn_ml9[2001:3000]), mean(tst_ml9[2001:3000])))

barplot(
  matrix(c(df$Model1, df$Model2, df$Model3, df$Model4, df$Model5, df$Model6, df$Model7, df$Model8, df$Model9), nrow = 2, ncol = 9),
  beside = TRUE,
  xlab='Model variations',
  names.arg=c(seq(1:9)),
  ylab='RSME',
  legend.text = c('train data', 'test data'),
  main="RSME where sigma = 5",
  col=c("lightblue","orange")
)
kable(t(df), col.names = c("Training", "Testing"))
```

## Discussion

Regardless of the $\sigma$ value, it turned out that `model 6` is superior to the other models. Because of the randomness of data split process, this method does not always guarantee to choose the best model all of the time. However, on average, this method succeeded to choose the correct model when simulated for 1000 times.

Note thay noise did not affect choosing the best model. While the overall RSME score has increased as $\sigma$ increases, the best model has remained the same.

One interesting point is that increasing the number of predictors does not always guarantee the better model. RSME score for model6 is lower than model7, model8, and model9.

Training measure always showed lower result than the testing experiment. That is beacuse interpolation is a safe and confident prediction while extrapolation is less safe and confident prediction.

For future work, we may discuss other metrics such as accuracy, f1 score, and cross entropy loss. 

# Simulation Study 3: Power (Graduate Students only)

In this simulation study we will investigate the **power** of the significance of regression test for simple linear regression. 

\[
H_0: \beta_{1} = 0 \ \text{vs} \ H_1: \beta_{1} \neq 0
\]

Recall, we had defined the *significance* level, $\alpha$, to be the probability of a Type I error.

\[
\alpha = P[\text{Reject } H_0 \mid H_0 \text{ True}] = P[\text{Type I Error}]
\]

Similarly, the probability of a Type II error is often denoted using $\beta$; however, this should not be confused with a regression parameter.

\[
\beta = P[\text{Fail to Reject } H_0 \mid H_1 \text{ True}] = P[\text{Type II Error}]
\]

*Power* is the probability of rejecting the null hypothesis when the null is not true, that is, the alternative is true and $\beta_{1}$ is non-zero.

\[
\text{Power} = 1 - \beta = P[\text{Reject } H_0 \mid H_1 \text{ True}]
\]

Essentially, power is the probability that a signal of a particular strength will be detected. Many things affect the power of a test. In this case, some of those are:

- Sample Size, $n$
- Signal Strength, $\beta_1$
- Noise Level, $\sigma$
- Significance Level, $\alpha$

We'll investigate the first three.

To do so we will simulate from the model

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$.

For simplicity, we will let $\beta_0 = 0$, thus $\beta_1$ is essentially controlling the amount of "signal." We will then consider different signals, noises, and sample sizes:

- $\beta_1 \in (-2, -1.9, -1.8, \ldots, -0.1, 0, 0.1, 0.2, 0.3, \ldots 1.9, 2)$
- $\sigma \in (1, 2, 4)$
- $n \in (10, 20, 30)$

We will hold the significance level constant at $\alpha = 0.05$.

Use the following code to generate the predictor values, `x`: values for different sample sizes.

```{r}
x_values = seq(0, 5, length = n)
```

For each possible $\beta_1$ and $\sigma$ combination, simulate from the true model at least $1000$ times. Each time, perform the significance of the regression test. To estimate the power with these simulations, and some $\alpha$, use

\[
\hat{\text{Power}} = \hat{P}[\text{Reject } H_0 \mid H_1 \text{ True}] = \frac{\text{# Tests Rejected}}{\text{# Simulations}}
\]

It is *possible* to derive an expression for power mathematically, but often this is difficult, so instead, we rely on simulation.

Create three plots, one for each value of $\sigma$. Within each of these plots, add a “power curve” for each value of $n$ that shows how power is affected by signal strength, $\beta_1$.

Potential discussions:

- How do $n$, $\beta_1$, and $\sigma$ affect power? Consider additional plots to demonstrate these effects.
- Are $1000$ simulations sufficient?

## Introduction

This simulation study examines the factors that affect power. It can be assumed that $n$, $\beta_1$, and $\sigma$ are the potential variables and power as reponse. Variables that affect power will have strong relationship with response.


## Method 
We will run three hypothesis tests
$$H_0: \beta_1 \propto \text{power vs } H_1: \text{not } H_0$$
$$H_0: \sigma \propto \text{power vs } H_1: \text{not } H_0$$
$$H_0: n \propto \text{power vs } H_1: \text{not } H_0$$
We will visualize the three the relationship between these three variables and power for demonstration.

```{r}
library(broom)
n = 10
beta_1 = seq(-2,2,.1)
pvals_n10_s1 = rep(rep(0, length(beta_1)), 1000)
pvals_n10_s2 = rep(rep(0, length(beta_1)), 1000)
pvals_n10_s4 = rep(rep(0, length(beta_1)), 1000)
pvals_n20_s1 = rep(rep(0, length(beta_1)), 1000)
pvals_n20_s2 = rep(rep(0, length(beta_1)), 1000)
pvals_n20_s4 = rep(rep(0, length(beta_1)), 1000)
pvals_n30_s1 = rep(rep(0, length(beta_1)), 1000)
pvals_n30_s2 = rep(rep(0, length(beta_1)), 1000)
pvals_n30_s4 = rep(rep(0, length(beta_1)), 1000)
```


```{r}
# n=10, sigma=1
  for (j in 1:1000){
    for (i in 1:length(beta_1)){
      x_values = seq(0, 5, length = n)
      df = data.frame(y = 0, x = x_values)
      epsilon = rnorm(n, mean = 0, sd = 1)
      df$y = beta_1[i]*df$x + epsilon
    
      model = lm(y~x, data=df)
      pvals_n10_s1[(j-1)*41+i] = glance(model)$p.value
    }
  }

# n=10, sigma=2
  for (j in 1:1000){
    for (i in 1:length(beta_1)){
      x_values = seq(0, 5, length = n)
      df = data.frame(y = 0, x = x_values)
      epsilon = rnorm(n, mean = 0, sd = 2)
      df$y = beta_1[i]*df$x + epsilon
    
      model = lm(y~x, data=df)
      pvals_n10_s2[(j-1)*41+i] = glance(model)$p.value
    }
  }

# n=10, sigma=3
  for (j in 1:1000){
    for (i in 1:length(beta_1)){
      x_values = seq(0, 5, length = n)
      df = data.frame(y = 0, x = x_values)
      epsilon = rnorm(n, mean = 0, sd = 4)
      df$y = beta_1[i]*df$x + epsilon
    
      model = lm(y~x, data=df)
      pvals_n10_s4[(j-1)*41+i] = glance(model)$p.value
    }
  }
```



```{r}
# n=20, sigma=1
  for (j in 1:1000){
    for (i in 1:length(beta_1)){
      x_values = seq(0, 5, length = n)
      df = data.frame(y = 0, x = x_values)
      epsilon = rnorm(n, mean = 0, sd = 1)
      df$y = beta_1[i]*df$x + epsilon
    
      model = lm(y~x, data=df)
      pvals_n20_s1[(j-1)*41+i] = glance(model)$p.value
    }
  }

# n=20, sigma=2
  for (j in 1:1000){
    for (i in 1:length(beta_1)){
      x_values = seq(0, 5, length = n)
      df = data.frame(y = 0, x = x_values)
      epsilon = rnorm(n, mean = 0, sd = 2)
      df$y = beta_1[i]*df$x + epsilon
    
      model = lm(y~x, data=df)
      pvals_n20_s2[(j-1)*41+i] = glance(model)$p.value
    }
  }

# n=20, sigma=4
  for (j in 1:1000){
    for (i in 1:length(beta_1)){
      x_values = seq(0, 5, length = n)
      df = data.frame(y = 0, x = x_values)
      epsilon = rnorm(n, mean = 0, sd = 4)
      df$y = beta_1[i]*df$x + epsilon
    
      model = lm(y~x, data=df)
      pvals_n20_s4[(j-1)*41+i] = glance(model)$p.value
    }
  }
```



```{r}
# n=30, sigma=s1
  for (j in 1:1000){
    for (i in 1:length(beta_1)){
      x_values = seq(0, 5, length = n)
      df = data.frame(y = 0, x = x_values)
      epsilon = rnorm(n, mean = 0, sd = 1)
      df$y = beta_1[i]*df$x + epsilon
    
      model = lm(y~x, data=df)
      pvals_n30_s1[(j-1)*41+i] = glance(model)$p.value
    }
  }

# n=30, sigma=2
  for (j in 1:1000){
    for (i in 1:length(beta_1)){
      x_values = seq(0, 5, length = n)
      df = data.frame(y = 0, x = x_values)
      epsilon = rnorm(n, mean = 0, sd = 2)
      df$y = beta_1[i]*df$x + epsilon
    
      model = lm(y~x, data=df)
      pvals_n30_s2[(j-1)*41+i] = glance(model)$p.value
    }
  }

# n=30, sigma=4
  for (j in 1:1000){
    for (i in 1:length(beta_1)){
      x_values = seq(0, 5, length = n)
      df = data.frame(y = 0, x = x_values)
      epsilon = rnorm(n, mean = 0, sd = 4)
      df$y = beta_1[i]*df$x + epsilon
    
      model = lm(y~x, data=df)
      pvals_n30_s4[(j-1)*41+i] = glance(model)$p.value
    }
  }
```



```{r echo=FALSE, message=FALSE}
# length(pvals_n10_s1[pvals_n10_s1 < 0.05])
# length(pvals_n10_s2[pvals_n10_s2 < 0.05])
# length(pvals_n10_s4[pvals_n10_s4 < 0.05])
# length(pvals_n20_s1[pvals_n20_s1 < 0.05])
# length(pvals_n20_s2[pvals_n20_s2 < 0.05])
# length(pvals_n20_s4[pvals_n20_s4 < 0.05])
# length(pvals_n30_s1[pvals_n30_s1 < 0.05])
# length(pvals_n30_s2[pvals_n30_s2 < 0.05])
# length(pvals_n30_s4[pvals_n30_s4 < 0.05])
```
## Results

The result below shows how increase in sample size(n) affects power.

```{r echo=FALSE}
q3df = data.frame(x1 = mean(pvals_n10_s1[pvals_n10_s1 < 0.05]),
                  x2 = mean(pvals_n20_s1[pvals_n20_s1 < 0.05]),
                  x3 = mean(pvals_n30_s1[pvals_n30_s1 < 0.05]))
par(mfrow = c(1,3))
barplot(
  matrix(c(q3df$x1, q3df$x2, q3df$x3), nrow = 1, ncol = 3),
  beside = TRUE,
  xlab='Sample size',
  names.arg=c('n=10',',n=20','n=30'),
  ylab='Power',
  # legend.text = c('train data', 'test data', 'd'),
  main= expression(sigma == 1),
  col=c("lightblue","orange","dodgerblue")
)
abline(h=mean(pvals_n10_s1[pvals_n10_s1 < 0.05]), col='red')

q3df = data.frame(x1 = mean(pvals_n10_s1[pvals_n10_s2 < 0.05]),
                  x2 = mean(pvals_n20_s1[pvals_n20_s2 < 0.05]),
                  x3 = mean(pvals_n30_s1[pvals_n30_s2 < 0.05]))

barplot(
  matrix(c(q3df$x1, q3df$x2, q3df$x3), nrow = 1, ncol = 3),
  beside = TRUE,
  xlab='Sample size',
  names.arg=c('n=10','n=20','n=30'),
  ylab='Power',
  # legend.text = c('train data', 'test data', 'd'),
  main= expression(sigma == 2),
  col=c("lightblue","orange","dodgerblue")
)
abline(h=q3df$x1, col='red')

q3df = data.frame(x1 = mean(pvals_n10_s1[pvals_n10_s4 < 0.05]),
                  x2 = mean(pvals_n20_s1[pvals_n20_s4 < 0.05]),
                  x3 = mean(pvals_n30_s1[pvals_n30_s4 < 0.05]))

barplot(
  matrix(c(q3df$x1, q3df$x2, q3df$x3), nrow = 1, ncol = 3),
  beside = TRUE,
  xlab='Sample size',
  names.arg=c('n=10','n=20','n=30'),
  ylab='Power',
  # legend.text = c('train data', 'test data', 'd'),
  main= expression(sigma == 4),
  col=c("lightblue","orange","dodgerblue")
)
abline(h=q3df$x1, col='red')
```

The plot below shows how increase in error($sigma$) affects power.

```{r echo=FALSE}
par(mfrow = c(1,3))

barplot(c(mean(pvals_n10_s1[pvals_n10_s1 < 0.05]), mean(pvals_n10_s1[pvals_n10_s2 < 0.05]), mean(pvals_n10_s1[pvals_n10_s4 < 0.05])),
          beside = TRUE,
  xlab='Standard Deviation',
  names.arg=c(expression(sigma == 1),expression(sigma == 2),expression(sigma == 4)),
  ylab='Power',
  # legend.text = c('train data', 'test data', 'd'),
  main= "Increase in Sigma where n=10",
  col=c("lightblue","orange","dodgerblue"))
abline(h=q3df$x1, col='red')

barplot(c(mean(pvals_n20_s1[pvals_n20_s1 < 0.05]), mean(pvals_n20_s1[pvals_n20_s2 < 0.05]), mean(pvals_n20_s1[pvals_n20_s4 < 0.05])),
          beside = TRUE,
  xlab='Standard Deviation',
  names.arg=c(expression(sigma == 1),expression(sigma == 2),expression(sigma == 4)),
  ylab='Power',
  # legend.text = c('train data', 'test data', 'd'),
  main= "Increase in Sigma where n=20",
  col=c("lightblue","orange","dodgerblue"))
abline(h=mean(pvals_n20_s1[pvals_n20_s4 < 0.05]), col='red')

barplot(c(mean(pvals_n10_s1[pvals_n30_s1 < 0.05]), mean(pvals_n10_s1[pvals_n30_s2 < 0.05]), mean(pvals_n10_s1[pvals_n30_s4 < 0.05])),
          beside = TRUE,
  xlab='Standard Deviation',
  names.arg=c(expression(sigma == 1),expression(sigma == 2),expression(sigma == 4)),
  ylab='Power',
  # legend.text = c('train data', 'test data', 'd'),
  main= "Increase in Sigma where n=30",
  col=c("lightblue","orange","dodgerblue"))
abline(h=mean(pvals_n10_s1[pvals_n30_s1 < 0.05]), col='red')
```

The last plot shows how change in $\beta$ affects power.

```{r echo=FALSE}
par(mfrow = c(1,3))

beta_01 = seq(01, length(pvals_n10_s1), 41)
beta_02 = seq(02, length(pvals_n10_s1), 41)
beta_03 = seq(03 , length(pvals_n10_s1), 41)
beta_04 = seq(04, length(pvals_n10_s1), 41)
beta_05 = seq(05, length(pvals_n10_s1), 41)
beta_06 = seq(06, length(pvals_n10_s1), 41)
beta_07 = seq(07, length(pvals_n10_s1), 41)
beta_08 = seq(08, length(pvals_n10_s1), 41)
beta_09 = seq(09 , length(pvals_n10_s1), 41)
beta_10 = seq( 10, length(pvals_n10_s1), 41)
beta_11 = seq( 11, length(pvals_n10_s1), 41)
beta_12 = seq( 12, length(pvals_n10_s1), 41)
beta_13 = seq( 13, length(pvals_n10_s1), 41)
beta_14 = seq( 14, length(pvals_n10_s1), 41)
beta_15 = seq( 15, length(pvals_n10_s1), 41)
beta_16 = seq( 16, length(pvals_n10_s1), 41)
beta_17 = seq( 17, length(pvals_n10_s1), 41)
beta_18 = seq( 18, length(pvals_n10_s1), 41)
beta_19 = seq( 19, length(pvals_n10_s1), 41)
beta_20 = seq( 20, length(pvals_n10_s1), 41)
beta_21 = seq( 21, length(pvals_n10_s1), 41)
beta_22 = seq( 22, length(pvals_n10_s1), 41)
beta_23 = seq( 23, length(pvals_n10_s1), 41)
beta_24 = seq( 24, length(pvals_n10_s1), 41)
beta_25 = seq( 25, length(pvals_n10_s1), 41)
beta_26 = seq( 26, length(pvals_n10_s1), 41)
beta_27 = seq( 27, length(pvals_n10_s1), 41)
beta_28 = seq( 28, length(pvals_n10_s1), 41)
beta_29 = seq( 29, length(pvals_n10_s1), 41)
beta_30 = seq( 30, length(pvals_n10_s1), 41)
beta_31 = seq( 31, length(pvals_n10_s1), 41)
beta_32 = seq( 32, length(pvals_n10_s1), 41)
beta_33 = seq( 33, length(pvals_n10_s1), 41)
beta_34 = seq( 34, length(pvals_n10_s1), 41)
beta_35 = seq( 35, length(pvals_n10_s1), 41)
beta_36 = seq( 36, length(pvals_n10_s1), 41)
beta_37 = seq( 37, length(pvals_n10_s1), 41)
beta_38 = seq( 38, length(pvals_n10_s1), 41)
beta_39 = seq( 39, length(pvals_n10_s1), 41)
beta_40 = seq( 40, length(pvals_n10_s1), 41)
beta_41 = seq( 41, length(pvals_n10_s1), 41)

n10_s1 = c(
mean(pvals_n10_s1[beta_01 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_02 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_03 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_04 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_05 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_06 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_07 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_08 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_09 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_10 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_11 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_12 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_13 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_14 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_15 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_16 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_17 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_18 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_19 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_20 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_21 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_22 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_23 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_24 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_25 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_26 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_27 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_28 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_29 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_30 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_31 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_32 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_33 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_34 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_35 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_36 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_37 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_38 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_39 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_40 & pvals_n10_s1 < 0.05]),
mean(pvals_n10_s1[beta_41 & pvals_n10_s1 < 0.05])
)
n20_s1 = c(
mean(pvals_n20_s1[beta_01 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_02 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_03 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_04 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_05 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_06 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_07 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_08 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_09 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_10 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_11 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_12 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_13 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_14 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_15 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_16 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_17 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_18 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_19 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_20 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_21 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_22 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_23 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_24 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_25 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_26 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_27 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_28 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_29 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_30 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_31 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_32 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_33 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_34 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_35 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_36 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_37 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_38 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_39 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_40 & pvals_n20_s1 < 0.05]),
mean(pvals_n20_s1[beta_41 & pvals_n20_s1 < 0.05])
)
n30_s1 = c(
mean(pvals_n30_s1[beta_01 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_02 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_03 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_04 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_05 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_06 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_07 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_08 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_09 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_10 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_11 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_12 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_13 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_14 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_15 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_16 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_17 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_18 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_19 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_20 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_21 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_22 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_23 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_24 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_25 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_26 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_27 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_28 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_29 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_30 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_31 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_32 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_33 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_34 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_35 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_36 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_37 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_38 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_39 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_40 & pvals_n30_s1 < 0.05]),
mean(pvals_n30_s1[beta_41 & pvals_n30_s1 < 0.05])
)

plot(n10_s1, xlab = expression(beta),
    ylab = 'power',
    main = 'power and beta',
    pch = 20,
    cex = 1,
    col = 'grey')
plot(n20_s1, xlab = expression(beta),
    ylab = 'power',
    main = 'power and beta',
    pch = 20,
    cex = 1,
    col = 'grey')
plot(n30_s1, xlab = expression(beta),
    ylab = 'power',
    main = 'power and beta',
    pch = 20,
    cex = 1,
    col = 'grey')

```

## Discussion

The first plot shows that sample size ($n$) does not affect power significantly. While a little discrepancy is observable, it does not seem to affect significantly. 

However, increase in $\sigma$ significantly affect power. Considering $\sigma$ is noise, it is expected that increase in noise will cause more error. 

The third plot showed that change in beta does not affect power. That is because power already assumes that $H_1$ is true. 

While 1000 is usually a large number enough for simulation, increased number of simulations is encouraged. That us because some inconsitency was found in plot 3. While $\sigma=4$ where n=30 is expected to have higher power, it turned out $\sigma=1$ where n=10 scored the highest score.